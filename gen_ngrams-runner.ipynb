{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator of (extended) (skippy) n-grams out of words or sentences\n",
    "\n",
    "developed by Kow Kuroda (kow.kuroda@gmail.com)\n",
    "\n",
    "This Jupyter notebook demonstrates how to use gen2_ngrams.py (or gen2_ngrams_cy.pyx) developed to enhance the usability of its predecessor \"gen_ngrams.py\".\n",
    "\n",
    "There are two main differences from its predecessor. First, gen_skippy_ngrams(..) generates extended skippy n-grams with \"extended = True\" option. Second, gen_skippy_ngrams(..) cann generate inclusive n-grams, thereby dispensing with incremental generation of n-grams from 1-grams.\n",
    "\n",
    "Limitations\n",
    "- Availablity of Cython-enhancement is limited. Apple Silicons like M1 and M2 (M3 is not tested yet) do not accept it, though it is available under Python 3.10 on M1.\n",
    "\n",
    "Creation\n",
    "- 2025/08/19\n",
    "\n",
    "Modifications\n",
    "- 2025/08/21 minor changes;\n",
    "- 2025/08/22 i) minor changes; Cython-enhancement was implemented;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda update -n base -c defaults conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cython の導入 (必要に応じて)\n",
    "#!conda uninstall cython -y # seems necessary in certain situations\n",
    "#!conda install cython -y\n",
    "## Try the following if the above fails\n",
    "#!pip install cython --upgrade --force-reinstall\n",
    "#!conda update -n base -c defaults conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip show cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cython を使うかどうか\n",
    "use_Cython = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cython extension の(再)構築が必要な場合は True に\n",
    "build_Cython_extension = False\n",
    "if use_Cython and build_Cython_extension:\n",
    "    !python setup.py clean build_ext --inplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cython 版の読込み\n",
    "if use_Cython:\n",
    "    try:\n",
    "        %reload_ext Cython\n",
    "    except ImportError:\n",
    "        %load_ext Cython\n",
    "    ## Apple Silicons like M1, M2 do not accept the following\n",
    "    import gen2_ngrams_cy as gen_ngrams\n",
    "else:\n",
    "    try:\n",
    "        import gen2_ngrams as gen_ngrams # gen_ngrams is now obsolete\n",
    "    except NameError:\n",
    "        import gen_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_words = True # if False, analyze sentential/phrasal objects\n",
    "\n",
    "## parameters for analysis\n",
    "if analyze_words:\n",
    "    segmenter: str = r\"\"\n",
    "    sep_local: str = \"\"\n",
    "else:\n",
    "    segmenter: str = r\" \"\n",
    "    sep_local: str = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "if analyze_words:\n",
    "    data_dir = 'data/words'\n",
    "    files = list(pathlib.Path(data_dir).glob('buddhist-listed2.txt'))\n",
    "else:\n",
    "    data_dir = 'data/phrases'\n",
    "    files = list(pathlib.Path(data_dir).glob('austen-j-sample100.txt'))\n",
    "##\n",
    "print(files)\n",
    "\n",
    "##\n",
    "file = files[0]\n",
    "source_name = file.stem\n",
    "print(f\"source_name: {source_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get data\n",
    "docs = file.read_text(encoding = 'utf-8').splitlines()\n",
    "\n",
    "## lowercase\n",
    "docs = [ doc.lower() for doc in docs if len(doc) > 0 ]\n",
    "print(docs[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of (extended) (skippy) n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## flags\n",
    "check: bool = False\n",
    "\n",
    "## saving results\n",
    "save_results: bool = False\n",
    "save_dir: str = \"saves\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### n-gram\n",
    "## n の最大値\n",
    "max_n_for_ngram: int = 5\n",
    "\n",
    "## n-gram\n",
    "ngram_is_inclusive = True\n",
    "skippy_means_extended = True\n",
    "\n",
    "## n-gram を文字列として生成するか否か\n",
    "generated_as_string: bool = True\n",
    "generated_as_list: bool = not(generated_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install pandas -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns0 = ['doc']\n",
    "columns1 = [ f\"xsk{i}g\" for i in range(1, max_n_for_ngram + 1)]\n",
    "columns2 = [ f\"sk{i}g\" for i in range(1, max_n_for_ngram + 1)]\n",
    "columns3 = [ f\"{i}g\" for i in range(1, max_n_for_ngram + 1)]\n",
    "\n",
    "used_columns = columns0 + columns1 + columns2 + columns3\n",
    "df = pd.DataFrame(columns = used_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate extended skippy n-grams\n",
    "import re\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Processing word {i} [use_Cython: {use_Cython}]: {doc}\")\n",
    "    word_segs = [ seg for seg in re.split(segmenter, doc) if len(seg) > 0 ]\n",
    "    for j in range(1, max_n_for_ngram + 1):\n",
    "        print(f\"generating extended skippy {j}-grams ...\")\n",
    "        ngrams = gen_ngrams.gen_skippy_ngrams(word_segs, j, extended = skippy_means_extended, inclusive = ngram_is_inclusive, sep = sep_local, as_list = generated_as_list, check = False)\n",
    "        if check:\n",
    "            print(ngrams)\n",
    "        ## update df\n",
    "        df.loc[i, f'xsk{j}g'] = ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate regular skippy n-grams\n",
    "import re\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"Processing word {i} [use_Cython: {use_Cython}]: {doc}\")\n",
    "    word_segs = [ seg for seg in re.split(segmenter, doc) if len(seg) > 0 ]\n",
    "    for j in range(1, max_n_for_ngram + 1):\n",
    "        print(f\"generating skippy {j}-grams ...\")\n",
    "        ngrams = gen_ngrams.gen_skippy_ngrams(word_segs, j, extended = skippy_means_extended, inclusive = ngram_is_inclusive, sep = sep_local, as_list = generated_as_list, check = False)\n",
    "        if check:\n",
    "            print(ngrams)\n",
    "        ## update df\n",
    "        df.loc[i, f'sk{j}g'] = ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate non-skippy n-grams\n",
    "import re\n",
    "for i, doc in enumerate(docs):\n",
    "    ## update df for word\n",
    "    df.loc[i,'doc'] = doc\n",
    "    ##\n",
    "    print(f\"Processing word {i} [use_Cython: {use_Cython}]: {doc}\")\n",
    "    word_segs = [ x for x in re.split(segmenter, doc) if len(x) > 0 ]\n",
    "    for j in range(1, max_n_for_ngram + 1):\n",
    "        print(f\"generating {j}-grams ...\")\n",
    "        ngrams = gen_ngrams.gen_ngrams(word_segs, j, inclusive = ngram_is_inclusive, sep = sep_local, as_list = generated_as_list, check = False)\n",
    "        if check:\n",
    "            print(ngrams)\n",
    "        ## update df\n",
    "        df.loc[i, f'{j}g'] = ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    file_name = f\"{save_dir}/{source_name}-reg-sk-xsk-df.csv\"\n",
    "    df.to_csv(file_name, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end of file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
